# Deep Learning with Label Noise / Noisy Labels

This repo consists of collection of papers and repos on the topic of deep learning by noisy labels. All methods listed below are briefly explained in the paper [Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey](https://arxiv.org/abs/1912.05170). More information about the topic can also be found on the survey.

|Year|Type|Conf|Repo|Title|
|----|----|----|----|-----|
|2019|SIW|NIPS||[Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting](https://arxiv.org/abs/1902.07379)|
|2019|RL |ICML||[On Symmetric Losses for Learning from Corrupted Labels](https://arxiv.org/abs/1901.09314)|
|2019|O  |ICLR|[Pt](https://github.com/orlitany/SOSELETO)|[SOSELETO: A Unified Approach to Transfer Learning and Training with Noisy Labels](https://arxiv.org/abs/1805.09622)|
|2019|LNC|ICLR||[An Energy-Based Framework for Arbitrary Label Noise Correction](https://openreview.net/forum?id=Hyxu6oAqYX)|
|2019|NC |NIPS||[Are Anchor Points Really Indispensable in Label-Noise Learning?](https://arxiv.org/abs/1906.00189)|
|2019|O  |NIPS|[Pt](https://github.com/snow12345/Combinatorial_Classification)|[Combinatorial Inference against Label Noise](https://papers.nips.cc/paper/8401-combinatorial-inference-against-label-noise)|
|2019|RL |NIPS||[L_DMI : A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise](https://arxiv.org/abs/1909.03388)|
|2019|O  |CVPR||[MetaCleaner: Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition](http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_MetaCleaner_Learning_to_Hallucinate_Clean_Representations_for_Noisy-Labeled_Visual_Recognition_CVPR_2019_paper.html)|
|2019|LNC|ICCV||[O2U-Net: A Simple Noisy Label Detection Approach for Deep Neural Networks](http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_O2U-Net_A_Simple_Noisy_Label_Detection_Approach_for_Deep_Neural_ICCV_2019_paper.pdf)|
|2019|SC |ICCV|[*](http://www.cbsr.ia.ac.cn/users/xiaobowang/)|[Co-Mining: Deep Face Recognition with Noisy Labels](http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Co-Mining_Deep_Face_Recognition_With_Noisy_Labels_ICCV_2019_paper.pdf)|
|2019|O  |     ||[NLNL: Negative Learning for Noisy Labels](https://arxiv.org/abs/1908.07387)
|2019|R  |     |[Pt](https://github.com/hendrycks/pre-training)|[Using Pre-Training Can Improve Model Robustness and Uncertainty](https://arxiv.org/abs/1901.09960)
|2019|SSL|     ||[Robust Learning Under Label Noise With Iterative Noise-Filtering](https://arxiv.org/abs/1906.00216)
|2019|ML |CVPR |[Pt](https://github.com/LiJunnan1992/MLNT)|[Learning to Learn from Noisy Labeled Data](https://arxiv.org/abs/1812.05214)
|2019|ML |     ||[Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels](https://arxiv.org/abs/1809.11008)
|2019|RL |     ||[Symmetric Cross Entropy for Robust Learning with Noisy Labels](https://arxiv.org/abs/1908.06112)
|2019|RL |     ||[Improved Mean Absolute Error for Learning Meaningful Patterns from Abnormal Training Data](https://arxiv.org/abs/1903.12141)
|2019|LQA|CVPR ||[Learning From Noisy Labels By Regularized Estimation Of Annotator Confusion](https://arxiv.org/abs/1902.03680)
|2019|SIW|CVPR |[Caffe](https://github.com/huangyangyu/NoiseFace)|[Noise-Tolerant Paradigm for Training Face Recognition CNNs](https://arxiv.org/abs/1903.10357)
|2019|SIW|ICML |[Pt](https://github.com/thulas/dac-label-noise)|[Combating Label Noise in Deep Learning Using Abstention](https://arxiv.org/abs/1905.10964)
|2019|SIW|     ||[Robust Learning at Noisy Labeled Medical Images: Applied to Skin Lesion Classification](https://arxiv.org/abs/1901.07759)
|2019|SC |ICML |[Keras](https://github.com/chenpf1025/noisy_label_understanding_utilizing)|[Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels](https://arxiv.org/abs/1905.05040)
|2019|SC |ICML |[Pt](https://github.com/bhanML/coteaching_plus)|[How does Disagreement Help Generalization against Label Corruption?](https://arxiv.org/abs/1901.04215)
|2019|SC |CVPR ||[Learning a Deep ConvNet for Multi-label Classification with Partial Labels](https://arxiv.org/abs/1902.09720)
|2019|SC |     ||[Curriculum Loss: Robust Learning and Generalization against Label Corruption](https://arxiv.org/abs/1905.10045)
|2019|SC |     ||[SELF: Learning to Filter Noisy Labels with Self-Ensembling](https://arxiv.org/abs/1910.01842)
|2019|LNC |CVPR |[Pt](https://github.com/jx-zhong-for-academic-purpose/GCN-Anomaly-Detection)|[Graph Convolutional Label Noise Cleaner: Train a Plug-and-play Action Classifier for Anomaly Detection](https://arxiv.org/abs/1903.07256)
|2019|LNC|ICCV ||[Photometric Transformer Networks and Label Adjustment for Breast Density Prediction](https://arxiv.org/abs/1905.02906)
|2019|LNC|CVPR |[Pt](https://github.com/yikun2019/PENCIL)|[Probabilistic End-to-end Noise Correction for Learning with Noisy Labels](https://arxiv.org/abs/1903.07788)
|2019|LNC|TGRS |[Matlab](https://github.com/junjun-jiang/RLPA)|[Hyperspectral image classification in the presence of noisy labels](https://arxiv.org/abs/1809.04212)
|2019|LNC|ICCV ||[Deep Self-Learning From Noisy Labels](https://arxiv.org/abs/1908.02160)
|2019|NC |AAAI |[Tf](https://github.com/Sunarker/Safeguarded-Dynamic-Label-Regression-for-Noisy-Supervision)|[Safeguarded Dynamic Label Regression for Noisy Supervision](https://arxiv.org/abs/1903.02152)
|2019|NC |ICML |[Pt](https://github.com/PaulAlbert31/LabelNoiseCorrection)|[Unsupervised Label Noise Modeling and Loss Correction](https://arxiv.org/abs/1904.11238)
|2018|O  |ECCV ||[Learning with Biased Complementary Labels](https://arxiv.org/abs/1711.09535)
|2018|O  |     ||[Robust Determinantal Generative Classifier for Noisy Labels and Adversarial Attacks](https://openreview.net/forum?id=rkle3i09K7)
|2018|R  |ICLR |[Keras](https://github.com/xingjunm/dimensionality-driven-learning)|[Dimensionality Driven Learning for Noisy Labels](https://arxiv.org/abs/1806.02612)
|2018|R  |ECCV ||[Deep bilevel learning](https://arxiv.org/abs/1809.01465)
|2018|SSL|WACV ||[A semi-supervised two-stage approach to learning from noisy labels](https://arxiv.org/abs/1802.02679)
|2018|ML |     ||[Improving Multi-Person Pose Estimation using Label Correction](https://arxiv.org/abs/1811.03331)
|2018|RL |NIPS ||[Generalized cross entropy loss for training deep neural networks with noisy labels](https://arxiv.org/abs/1805.07836)
|2018|LQA|ICLR |[Repo](https://github.com/khetan2/MBEM)|[Learning From Noisy Singly-Labeled Data](https://arxiv.org/abs/1712.04577)
|2018|LQA|AAAI ||[Deep learning from crowds](https://arxiv.org/abs/1709.01779)
|2018|SIW|CVPR |[Repo](https://github.com/YisenWang/Iterative_learning)|[Iterative Learning With Open-Set Noisy Labels](https://arxiv.org/abs/1804.00092)               
|2018|SIW|     |[Tf](https://github.com/uber-research/learning-to-reweight-examples)|[Learning to Reweight Examples for Robust Deep Learning](https://arxiv.org/abs/1803.09050)
|2018|SIW|CVPR |[Tf](https://github.com/kuanghuei/clean-net)|[Cleannet: Transfer Learning for Scalable Image Classifier Training with Label Noise](https://arxiv.org/abs/1711.07131)
|2018|SIW|     ||[ChoiceNet: Robust Learning by Revealing Output Correlations](https://arxiv.org/abs/1805.06431)
|2018|SIW|IEEE ||[Multiclass Learning with Partially Corrupted Labels](https://ieeexplore.ieee.org/abstract/document/7929355/)
|2018|SC |NIPS |[Pt](https://github.com/bhanML/Co-teaching)|[Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels](https://arxiv.org/abs/1804.06872)
|2018|SC |IEEE ||[Progressive Stochastic Learning for Noisy Labels](https://ieeexplore.ieee.org/document/8281022)
|2018|SC |ECCV |[Sklearn](https://github.com/MalongTech/research-curriculumnet)|[Curriculumnet: Weakly supervised learning from large-scale web images](https://arxiv.org/abs/1808.01097)
|2018|LNC|CVPR | [Chainer](https://github.com/DaikiTanaka-UT/JointOptimization)|[Joint Optimization Framework for Learning with Noisy Labels](https://arxiv.org/abs/1803.11364)
|2018|LNC|TIFS | [Pt](https://github.com/AlfredXiangWu/LightCNN), [Caffe](https://github.com/AlfredXiangWu/face_verification_experiment), [Tf](https://github.com/yxu0611/Tensorflow-implementation-of-LCNN)|[A light CNN for deep face representation with noisy labels](https://arxiv.org/abs/1511.02683)
|2018|LNC|WACV ||[Iterative cross learning on noisy labels](https://ieeexplore.ieee.org/document/8354192)
|2018|NC |NIPS |[Pt](https://github.com/mmazeika/glc)|[Using trusted data to train deep networks on labels corrupted by severe noise](https://arxiv.org/abs/1802.05300)
|2018|NC |ISBI ||[Training a neural network based on unreliable human annotation of medical images](https://ieeexplore.ieee.org/document/8363518/)
|2018|NC |IEEE ||[Deep learning from noisy image labels with quality embedding](https://arxiv.org/abs/1711.00583)
|2018|NC |NIPS |[Tf](https://github.com/bhanML/Masking) | [Masking: A new perspective of noisy supervision](https://arxiv.org/abs/1805.08193)
|2017|O  |     ||[Learning with Auxiliary Less-Noisy Labels](https://ieeexplore.ieee.org/document/7448430)
|2017|R  |     ||[Regularizing neural networks by penalizing confident output distributions](https://arxiv.org/abs/1701.06548)
|2017|R  |     |[Pt](https://github.com/facebookresearch/mixup-cifar10)|[mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412)
|2017|MIL|CVPR ||[Attend in groups: a weakly-supervised deep learning framework for learning from web data](https://arxiv.org/abs/1611.09960)
|2017|ML |ICCV ||[Learning from Noisy Labels with Distillation](https://arxiv.org/abs/1703.02391)
|2017|ML |     ||[Avoiding your teacher's mistakes: Training neural networks with controlled weak supervision](https://arxiv.org/abs/1711.00313)
|2017|ML |     ||[Learning to Learn from Weak Supervision by Full Supervision](https://arxiv.org/abs/1711.11383)
|2017|RL |AAAI ||[Robust Loss Functions under Label Noise for Deep Neural](https://arxiv.org/abs/1712.09482)
|2017|LQA|ICLR ||[Who Said What: Modeling Individual Labelers Improves Classification](https://arxiv.org/abs/1703.08774)
|2017|LQA|CVPR ||[Lean crowdsourcing: Combining humans and machines in an online system](http://ieeexplore.ieee.org/document/8100130/)
|2017|SC |NIPS |[Tf](https://github.com/emalach/UpdateByDisagreement)|[Decoupling" when to update" from" how to update"](https://arxiv.org/abs/1706.02613)
|2017|SC |NIPS |[Tf*](https://github.com/songhwanjun/ActiveBias)|[Active bias: Training more accurate neural networks by emphasizing high variance samples](https://arxiv.org/abs/1704.07433)
|2017|SC |     |[Tf](https://github.com/google/mentornet)|[MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels](https://arxiv.org/abs/1712.05055)
|2017|SC |     |[Sklearn](https://github.com/cgnorthcutt/rankpruning)|[Learning with confident examples: Rank pruning for robust classification with noisy labels](https://arxiv.org/abs/1705.01936)
|2017|SC |NIPS ||[Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks](https://arxiv.org/abs/1706.00038)
|2017|LNC|IEEE ||[Self-Error-Correcting Convolutional Neural Network for Learning with Noisy Labels](http://ieeexplore.ieee.org/document/7961730/)
|2017|LNC|IEEE ||[Improving crowdsourced label quality using noise correction](https://ieeexplore.ieee.org/document/7885126/)
|2017|LNC|     ||[Fidelity-weighted learning](https://arxiv.org/abs/1711.02799)
|2017|LNC|CVPR ||[Learning From Noisy Large-Scale Datasets With Minimal Supervision](https://arxiv.org/abs/1701.01619)
|2017|NC |CVPR |[Keras](https://github.com/giorgiop/loss-correction)|[Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach](https://arxiv.org/abs/1609.03683)
|2017|NC |ICLR |[Keras](https://github.com/udibr/noisy_labels)|[Training Deep Neural-Networks Using a Noise Adaptation Layer](https://openreview.net/forum?id=H12GRgcxg)
|2016|EM |KBS  ||[A robust multi-class AdaBoost algorithm for mislabeled noisy data](https://dl.acm.org/citation.cfm?id=2932672)
|2016|R  |CVPR ||[Rethinking the inception architecture for computer vision](https://arxiv.org/abs/1512.00567)
|2016|SSL|AAAI ||[Robust semi-supervised learning through label aggregation](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12312)
|2016|ML |NC   ||[Noise detection in the Meta-Learning Level](https://www.sciencedirect.com/science/article/abs/pii/S0925231215005482)
|2016|RL |     ||[On the convergence of a family of robust losses for stochastic gradient descent](https://arxiv.org/abs/1605.01623)
|2016|RL |ICML ||[Loss factorization, weakly supervised learning and label noise robustness](https://arxiv.org/abs/1602.02450)
|2016|SIW|ICLR |[Matlab](https://github.com/azadis/AIR)|[Auxiliary image regularization for deep cnns with noisy labels](https://arxiv.org/abs/1511.07069)
|2016|SIW|CVPR |[Caffe](https://github.com/imisra/latent-noise-icnm)|[Seeing Through the Human Reporting Bias: Visual Classifiers From Noisy Human-Centric Labels](https://arxiv.org/abs/1512.06974)
|2016|SC |ECCV |[Repo](https://github.com/google/goldfinch)|[The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition](https://arxiv.org/abs/1511.06789)
|2016|NC |ICDM |[Matlab](https://github.com/ijindal/Noisy_Dropout_regularization)|[Learning deep networks from noisy labels with dropout regularization](https://arxiv.org/abs/1705.03419)
|2016|NC |CASSP|[Keras](https://github.com/udibr/noisy_labels) |[Training deep neural-networks based on unreliable labels](https://ieeexplore.ieee.org/document/7472164)
|2015|O  |     ||[Learning discriminative reconstructions for unsupervised outlier removal](https://ieeexplore.ieee.org/document/7410534)
|2015|EM |     ||[Rboost: label noise-robust boosting algorithm based on a nonconvex loss function and the numerically stable base learners](http://ieeexplore.ieee.org/document/7273923/)
|2015|MIL|CVPR ||[Visual recognition by learning from web data: A weakly supervised domain generalization approach](http://ieeexplore.ieee.org/document/7298894/)
|2015|RL |NIPS ||[Learning with symmetric label noise: The importance of being unhinge](https://arxiv.org/abs/1505.07634)
|2015|RL |NC   ||[Making risk minimization tolerant to label noise](https://arxiv.org/abs/1403.3610)
|2015|LQA|     ||[Deep classifiers from image tags in the wild](https://dl.acm.org/citation.cfm?id=2814821)
|2015|SIW|TPAMI|[Pt](https://github.com/xiaoboxia/Classification-with-noisy-labels-by-importance-reweighting)|[Classification with noisy labels by importance reweighting](https://arxiv.org/pdf/1411.7718)
|2015|SC |ICCV |[Website](http://xinleic.xyz/web.html)|[Webly supervised learning of convolutional networks](https://arxiv.org/abs/1505.01554)
|2015|NC |CVPR |[Caffe](https://github.com/Cysu/noisy_label) | [Learning From Massive Noisy Labeled Data for Image Classification](https://ieeexplore.ieee.org/document/7298885)
|2015|NC |ICLR ||[Training Convolutional Networks with Noisy Labels](https://arxiv.org/abs/1406.2080)
|2014|R  |     ||[Explaining and harnessing adversarial examples](https://arxiv.org/abs/1412.6572)
|2014|R  |JMLR ||[Dropout: a simple way to prevent neural networks from overfitting](http://jmlr.org/papers/v15/srivastava14a.html)
|2014|SC |     |[Keras](https://github.com/dwright04/Noisy-Labels-with-Bootstrapping)|[Training Deep Neural Networks on Noisy Labels with Bootstrapping](https://arxiv.org/abs/1412.6596)
|2014|NC |     ||[Learning from Noisy Labels with Deep Neural Networks](https://arxiv.org/abs/1406.2080)
|2014|LQA|     ||[Learning from multiple annotators with varying expertise](https://link.springer.com/article/10.1007/s10994-013-5412-1)
|2013|EM |     ||[Boosting in the presence of label noise](https://arxiv.org/abs/1309.6818)
|2013|RL |NIPS ||[Learning with Noisy Labels](https://papers.nips.cc/paper/5073-learning-with-noisy-labels)
|2013|RL |IEEE ||[Noise tolerance under risk minimization](https://arxiv.org/pdf/1109.5231)
|2012|EM |     ||[A noise-detection based AdaBoost algorithm for mislabeled data](https://www.sciencedirect.com/science/article/pii/S0031320312002130)
|2012|RL |ICML ||[Learning to Label Aerial Images from Noisy Data](https://dl.acm.org/citation.cfm?id=3042603)
|2011|EM |     ||[An empirical comparison of two boosting algorithms on real data sets with artificial class noise](https://link.springer.com/chapter/10.1007/978-3-642-22418-8_4)
|2009|LQA|     ||[Supervised learning from multiple experts: whom to trust when everyone lies a bit](https://dl.acm.org/citation.cfm?id=1553488)
|2008|LQA|NIPS ||[Whose vote should count more: Optimal integration of labels from labelers of unknown expertise](https://papers.nips.cc/paper/3644-whose-vote-should-count-more-optimal-integration-of-labels-from-labelers-of-unknown-expertise)
|2006|RL |JASA ||[Convexity, classification, and risk bounds](http://statistics.berkeley.edu/sites/default/files/tech-reports/638.pdf)
|2000|EM |     ||[An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization](https://link.springer.com/article/10.1023/A:1007607513941)

List of papers that shed light to label noise phenomenon for deep learning:

|Title                                                                                                                  | Year |
|-----                                                                                                                  | ---- |
|[Investigating CNNs' Learning Representation Under Label Noise](https://openreview.net/pdf?id=H1xmqiAqFm)              | 2019 |
|[Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey](https://arxiv.org/abs/1912.05170)  | 2019 |
|[How Do Neural Networks Overcome Label Noise?](https://openreview.net/forum?id=ryu4RYJPM)                              | 2018 |
|[Deep Learning is Robust to Massive Label Noise](https://arxiv.org/abs/1705.10694)                                     | 2018 |
|[A closer look at memorization in deep networks](https://arxiv.org/abs/1706.05394)                                     | 2017 |
|[Deep Nets Don't Learn via Memorization](https://openreview.net/forum?id=rJv6ZgHYg)                                    | 2017 |
|[On the robustness of convnets to training on noisy labels](https://www.semanticscholar.org/paper/On-the-Robustness-of-ConvNets-to-Training-on-Noisy-Stanford/56c060905d3be9b7ea877e58c36d6856ee1205dd)  | 2017 |
|[A study of the effect of different types of noise on the precision of supervised learning techniques](https://link.springer.com/article/10.1007/s10462-010-9156-z) | 2017 |
|[Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530)         | 2016 |
|[A comprehensive introduction to label noise](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2014-10.pdf)| 2014 |
|[Classification in the Presence of Label Noise: a Survey](https://ieeexplore.ieee.org/document/6685834)    | 2014 |
|[Class noise and supervised learning in medical domains: The effect of feature extraction](https://ieeexplore.ieee.org/abstract/document/1647654/) | 2006 |
|[Class noise vs. attribute noise: A quantitative study](https://link.springer.com/article/10.1007/s10462-004-0751-8) | 2004 |

List of works under label noise beside classification

|Title                                                                                                          |Year|
|-----                                                                                                          |----|
|[Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations](https://arxiv.org/abs/1904.07934) |2019|
|[Improving Semantic Segmentation via Video Propagation and Label Relaxation](https://arxiv.org/abs/1812.01593) |2018|
|[Learning from weak and noisy labels for semantic segmentation](https://ieeexplore.ieee.org/document/7450177)  |2016|
|[Robustness of conditional GANs to noisy labels](https://arxiv.org/abs/1811.03205)                             |2018|
|[Label-Noise Robust Generative Adversarial Networks](https://arxiv.org/abs/1811.11165)                         |2018|

Sources on web
* [Noisy-Labels-Problem-Collection](https://github.com/GuokaiLiu/Noisy-Labels-Problem-Collection)
* [Learning-with-Label-Noise](https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise)

Abbreviations for noise types are:
* NC -> Noisy Channel
* LNC -> Label Noise Cleansing
* SC -> Sample Choosing
* SIW -> Sample Importance Weighting
* LQA -> Labeler quality assesment
* RL -> Robust Losses
* ML -> Meta Learning
* MIL -> Multiple Instance Learning
* SSL -> Semi Supervised Learning
* R -> Regularizers
* EM -> Ensemble Methods
* O -> Others

Other abbreviations:
* NC -> Neurocomputing
* Tf -> Tensorflow
* Pt -> PyTorch

Starred (*) repos means code is unoffical!