# Deep Learning with Label Noise / Noisy Labels

This repo consists of collection of papers and repos on the topic of deep learning by noisy labels. All methods listed below are briefly explained in the paper [Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey]() (Survey link will be added soon!). More information about the topic can also be found on the survey.

|Year|Type          |Conf |Repo|Title|
|----|--------------    |-----|----|-----|
|2019|O  |     ||[NLNL: Negative Learning for Noisy Labels]()
|2019|R  |     |[Pt](https://github.com/hendrycks/pre-training)|[Using Pre-Training Can Improve Model Robustness and Uncertainty]()
|2019|SSL|     ||[Robust Learning Under Label Noise With Iterative Noise-Filtering]()
|2019|ML |CVPR |[Pt](https://github.com/LiJunnan1992/MLNT)|[Learning to Learn from Noisy Labeled Data]()
|2019|ML |     ||[Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels]()
|2019|RL |     ||[Symmetric Cross Entropy for Robust Learning with Noisy Labels]()
|2019|RL |     ||[Improved Mean Absolute Error for Learning Meaningful Patterns from Abnormal Training Data]()
|2019|LQA|CVPR ||[Learning From Noisy Labels By Regularized Estimation Of Annotator Confusion]()
|2019|SIW|CVPR |[Caffe](https://github.com/huangyangyu/NoiseFace)|[Noise-Tolerant Paradigm for Training Face Recognition CNNs]()
|2019|SIW|ICML |[Pt](https://github.com/thulas/dac-label-noise)|[Combating Label Noise in Deep Learning Using Abstention]()
|2019|SIW|     ||[Robust Learning at Noisy Labeled Medical Images: Applied to Skin Lesion Classification]()
|2019|SC |ICML |[Keras](https://github.com/chenpf1025/noisy_label_understanding_utilizing)|[Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels]()
|2019|SC |ICML |[Pt](https://github.com/bhanML/coteaching_plus)|[How does Disagreement Help Generalization against Label Corruption?]()
|2019|SC |CVPR ||[Learning a Deep ConvNet for Multi-label Classification with Partial Labels]()
|2019|SC |     ||[Curriculum Loss: Robust Learning and Generalization against Label Corruption]()
|2019|SC |     ||[SELF: Learning to Filter Noisy Labels with Self-Ensembling]()
|2019|LNC |CVPR |[Pt](https://github.com/jx-zhong-for-academic-purpose/GCN-Anomaly-Detection)|[Graph Convolutional Label Noise Cleaner: Train a Plug-and-play Action Classifier for Anomaly Detection]()
|2019|LNC|ICCV ||[Photometric Transformer Networks and Label Adjustment for Breast Density Prediction]()
|2019|LNC|CVPR |[Pt](https://github.com/yikun2019/PENCIL)|[Probabilistic End-to-end Noise Correction for Learning with Noisy Labels]()
|2019|LNC|TGRS |[Matlab](https://github.com/junjun-jiang/RLPA)|[Hyperspectral image classification in the presence of noisy labels]()
|2019|LNC|ICCV ||[Deep Self-Learning From Noisy Labels]()
|2019|NC |AAAI |[Tf](https://github.com/Sunarker/Safeguarded-Dynamic-Label-Regression-for-Noisy-Supervision)|[Safeguarded Dynamic Label Regression for Noisy Supervision]()
|2019|NC |ICML |[Pt](https://github.com/PaulAlbert31/LabelNoiseCorrection)|[Unsupervised Label Noise Modeling and Loss Correction]()
|2018|O  |ECCV ||[Learning with Biased Complementary Labels]()
|2018|O  |     ||[Robust Determinantal Generative Classifier for Noisy Labels and Adversarial Attacks]()
|2018|R  |ICLR |[Keras](https://github.com/xingjunm/dimensionality-driven-learning)|[Dimensionality Driven Learning for Noisy Labels]()
|2018|R  |ECCV ||[Deep bilevel learning]()
|2018|SSL|WACV ||[A semi-supervised two-stage approach to learning from noisy labels]()
|2018|ML |     ||[Improving Multi-Person Pose Estimation using Label Correction]()
|2018|RL |NIPS ||[Generalized cross entropy loss for training deep neural networks with noisy labels]()
|2018|LQA|ICLR |[Repo](https://github.com/khetan2/MBEM)|[Learning From Noisy Singly-Labeled Data]()
|2018|LQA|AAAI ||[Deep learning from crowds]()
|2018|SIW|CVPR |[Repo](https://github.com/YisenWang/Iterative_learning)|[Iterative Learning With Open-Set Noisy Labels]()               
|2018|SIW|     |[Tf](https://github.com/uber-research/learning-to-reweight-examples)|[Learning to Reweight Examples for Robust Deep Learning]()
|2018|SIW|CVPR |[Tf](https://github.com/kuanghuei/clean-net)|[Cleannet: Transfer Learning for Scalable Image Classifier Training with Label Noise]()
|2018|SIW|     ||[ChoiceNet: Robust Learning by Revealing Output Correlations]()
|2018|SIW|IEEE ||[Multiclass Learning with Partially Corrupted Labels]()
|2018|SC |NIPS |[Pt](https://github.com/bhanML/Co-teaching)|[Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels]()
|2018|SC |IEEE ||[Progressive Stochastic Learning for Noisy Labels]()
|2018|SC |ECCV |[Sklearn](https://github.com/MalongTech/research-curriculumnet)|[Curriculumnet: Weakly supervised learning from large-scale web images]()
|2018|LNC|CVPR | [Chainer](https://github.com/DaikiTanaka-UT/JointOptimization)|[Joint Optimization Framework for Learning with Noisy Labels]()
|2018|LNC|TIFS | [Pt](https://github.com/AlfredXiangWu/LightCNN), [Caffe](https://github.com/AlfredXiangWu/face_verification_experiment), [Tf](https://github.com/yxu0611/Tensorflow-implementation-of-LCNN)|[A light CNN for deep face representation with noisy labels]()
|2018|LNC|WACV ||[Iterative cross learning on noisy labels]()
|2018|NC |NIPS |[Pt](https://github.com/mmazeika/glc)|[Using trusted data to train deep networks on labels corrupted by severe noise]()
|2018|NC |ISBI ||[Training a neural network based on unreliable human annotation of medical images]()
|2018|NC |IEEE ||[Deep learning from noisy image labels with quality embedding]()
|2018|NC |NIPS |[Tf](https://github.com/bhanML/Masking) | [Masking: A new perspective of noisy supervision]()
|2017|O  |     ||[Learning with Auxiliary Less-Noisy Labels]()
|2017|R  |     ||[Regularizing neural networks by penalizing confident output distributions]()
|2017|R  |     |[Pt](https://github.com/facebookresearch/mixup-cifar10)|[mixup: Beyond Empirical Risk Minimization]()
|2017|MIL|CVPR ||[Attend in groups: a weakly-supervised deep learning framework for learning from web data]()
|2017|ML |ICCV ||[Learning from Noisy Labels with Distillation]()
|2017|ML |     ||[Avoiding your teacher's mistakes: Training neural networks with controlled weak supervision]()
|2017|ML |     ||[Learning to Learn from Weak Supervision by Full Supervision]()
|2017|RL |AAAI ||[Robust Loss Functions under Label Noise for Deep Neural]()
|2017|LQA|ICLR ||[Who Said What: Modeling Individual Labelers Improves Classification]()
|2017|LQA|CVPR ||[Lean crowdsourcing: Combining humans and machines in an online system]()
|2017|SC |NIPS |[Tf](https://github.com/emalach/UpdateByDisagreement)|[Decoupling" when to update" from" how to update"]()
|2017|SC |NIPS |[Tf*](https://github.com/songhwanjun/ActiveBias)|[Active bias: Training more accurate neural networks by emphasizing high variance samples]()
|2017|SC |     |[Tf](https://github.com/google/mentornet)|[MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels]()
|2017|SC |     |[Sklearn](https://github.com/cgnorthcutt/rankpruning)|[Learning with confident examples: Rank pruning for robust classification with noisy labels]()
|2017|SC |NIPS ||[Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks]()
|2017|LNC|IEEE ||[Self-Error-Correcting Convolutional Neural Network for Learning with Noisy Labels]()
|2017|LNC|IEEE ||[Improving crowdsourced label quality using noise correction]()
|2017|LNC|     ||[Fidelity-weighted learning]()
|2017|LNC|CVPR ||[Learning From Noisy Large-Scale Datasets With Minimal Supervision]()
|2017|NC |CVPR |[Keras](https://github.com/giorgiop/loss-correction)|[Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach]()
|2017|NC |ICLR |[Keras](https://github.com/udibr/noisy_labels)|[Training Deep Neural-Networks Using a Noise Adaptation Layer]()
|2016|EM |KBS  ||[A robust multi-class AdaBoost algorithm for mislabeled noisy data]()
|2016|R  |CVPR ||[Rethinking the inception architecture for computer vision]()
|2016|SSL|AAAI ||[Robust semi-supervised learning through label aggregation]()
|2016|ML |NC   ||[Noise detection in the Meta-Learning Level]()
|2016|RL |     ||[On the convergence of a family of robust losses for stochastic gradient descent]()
|2016|RL |ICML ||[Loss factorization, weakly supervised learning and label noise robustness]()
|2016|SIW|ICLR |[Matlab](https://github.com/azadis/AIR)|[Auxiliary image regularization for deep cnns with noisy labels]()
|2016|SIW|CVPR |[Caffe](https://github.com/imisra/latent-noise-icnm)|[Seeing Through the Human Reporting Bias: Visual Classifiers From Noisy Human-Centric Labels]()
|2016|SC |ECCV |[Repo](https://github.com/google/goldfinch)|[The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition]()
|2016|NC |ICDM |[Matlab](https://github.com/ijindal/Noisy_Dropout_regularization)|[Learning deep networks from noisy labels with dropout regularization]()
|2016|NC |CASSP|[Keras](https://github.com/udibr/noisy_labels) |[Training deep neural-networks based on unreliable labels]()
|2015|O  |     ||[Learning discriminative reconstructions for unsupervised outlier removal]()
|2015|EM |     ||[Rboost: label noise-robust boosting algorithm based on a nonconvex loss function and the numerically stable base learners]()
|2015|MIL|CVPR ||[Visual recognition by learning from web data: A weakly supervised domain generalization approach]()
|2015|RL |NIPS ||[Learning with symmetric label noise: The importance of being unhinge]()
|2015|RL |NC   ||[Making risk minimization tolerant to label noise]()
|2015|LQA|     ||[Deep classifiers from image tags in the wild]()
|2015|SIW|TPAMI|[Pt](https://github.com/xiaoboxia/Classification-with-noisy-labels-by-importance-reweighting)|[Classification with noisy labels by importance reweighting]()
|2015|SC |ICCV |[Website](http://xinleic.xyz/web.html)|[Webly supervised learning of convolutional networks]()
|2015|NC |CVPR |[Caffe](https://github.com/Cysu/noisy_label) | [Learning From Massive Noisy Labeled Data for Image Classification]()
|2015|NC |ICLR ||[Training Convolutional Networks with Noisy Labels]()
|2014|R  |     ||[Explaining and harnessing adversarial examples]()
|2014|R  |JMLR ||[Dropout: a simple way to prevent neural networks from overfitting]()
|2014|SC |     |[Keras](https://github.com/dwright04/Noisy-Labels-with-Bootstrapping)|[Training Deep Neural Networks on Noisy Labels with Bootstrapping]()
|2014|NC |     ||[Learning from Noisy Labels with Deep Neural Networks]()
|2014|LQA|     ||[Learning from multiple annotators with varying expertise]()
|2013|EM |     ||[Boosting in the presence of label noise]()
|2013|RL |NIPS ||[Learning with Noisy Labels]()
|2013|RL |IEEE ||[Noise tolerance under risk minimization]()
|2012|EM |     ||[A noise-detection based AdaBoost algorithm for mislabeled data]()
|2012|RL |ICML ||[Learning to Label Aerial Images from Noisy Data]()
|2011|EM |     ||[An empirical comparison of two boosting algorithms on real data sets with artificial class noise]()
|2009|LQA|     ||[Supervised learning from multiple experts: whom to trust when everyone lies a bit]()
|2008|LQA|NIPS | |[Whose vote should count more: Optimal integration of labels from labelers of unknown expertise]()
|2006|RL |JASA ||[Convexity, classification, and risk bounds]()
|2000|EM |     ||[An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization]()

List of papers that shed light to label noise phenomenon for deep learning:

|Title                                                                                                      | Year |
|-----                                                                                                      | ---- |
|[How Do Neural Networks Overcome Label Noise?]()                                                           | 2018 |
|[Deep Learning is Robust to Massive Label Noise]()                                                         | 2018 |
|[A closer look at memorization in deep networks]()                                                         | 2017 |
|[Deep Nets Don't Learn via Memorization]()                                                                 | 2017 |
|[On the robustness of convnets to training on noisy labels]()                                              | 2017 |
|[A study of the effect of different types of noise on the precision of supervised learning techniques]()   | 2017 |
|[Understanding deep learning requires rethinking generalization]()                                         | 2016 |
|[A comprehensive introduction to label noise]()                                                            | 2014 |
|[Classification in the Presence of Label Noise: a Survey]()                                                | 2014 |
|[Class noise and supervised learning in medical domains: The effect of feature extraction]()               | 2006 |
|[Class noise vs. attribute noise: A quantitative study]()                                                  | 2004 |

List of works under label noise beside classification

|Title                                                                          |Year|
|-----                                                                          |----|
|[Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations]() |2019|
|[Improving Semantic Segmentation via Video Propagation and Label Relaxation]() |2018|
|[Learning from weak and noisy labels for semantic segmentation]()              |2016|
|[Robustness of conditional GANs to noisy labels]()                             |2018|
|[Label-Noise Robust Generative Adversarial Networks]()                         |2018|

Sources on web
* [Noisy-Labels-Problem-Collection](https://github.com/GuokaiLiu/Noisy-Labels-Problem-Collection)
* [Learning-with-Label-Noise](https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise)

Abbreviations for noise types are:
* NC -> Noisy Channel
* LNC -> Label Noise Cleansing
* SC -> Sample Choosing
* SIW -> Sample Importance Weighting
* LQA -> Labeler quality assesment
* RL -> Robust Losses
* ML -> Meta Learning
* MIL -> Multiple Instance Learning
* SSL -> Semi Supervised Learning
* R -> Regularizers
* EM -> Ensemble Methods
* O -> Others

Other abbreviations:
* NC -> Neurocomputing
* Tf -> Tensorflow
* Pt -> PyTorch

Starred (*) repos means code is unoffical!