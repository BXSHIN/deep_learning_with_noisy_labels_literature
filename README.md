# Deep Learning with Label Noise / Noisy Labels

This repo consists of collection of papers and repos on the topic of deep learning by noisy labels. All methods listed below are briefly explained in the paper [Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey]() (Survey link will be added soon!). More information about the topic can also be found on the survey.

|Year|Type          |Conf |Repo|Title|
|----|--------------    |-----|----|-----|
|2000|Ensemble Learning |     ||[An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization]()
|2006|Robust Losses     |JASA ||[Convexity, classification, and risk bounds]()
|2008|Labeler Quality   |NIPS | |[Whose vote should count more: Optimal integration of labels from labelers of unknown expertise]()
|2009|Labeler Quality   |     ||[Supervised learning from multiple experts: whom to trust when everyone lies a bit]()
|2011|Ensemble Learning |     ||[An empirical comparison of two boosting algorithms on real data sets with artificial class noise]()
|2012|Robust Losses     |ICML ||[Learning to Label Aerial Images from Noisy Data]()
|2012|Ensemble Learning |     ||[A noise-detection based AdaBoost algorithm for mislabeled data]()
|2013|Robust Losses     |IEEE ||[Noise tolerance under risk minimization]()
|2013|Robust Losses     |NIPS ||[Learning with Noisy Labels]()
|2013|Ensemble Learning |     ||[Boosting in the presence of label noise]()
|2014|Labeler Quality   |     ||[Learning from multiple annotators with varying expertise]()
|2014|Noisy Channel     |     ||[Learning from Noisy Labels with Deep Neural Networks]()
|2014|Sample Choosing   |     |[Keras](https://github.com/dwright04/Noisy-Labels-with-Bootstrapping)|[Training Deep Neural Networks on Noisy Labels with Bootstrapping]()
|2014|Regularizer       |JMLR ||[Dropout: a simple way to prevent neural networks from overfitting]()
|2014|Regularizer       |     ||[Explaining and harnessing adversarial examples]()
|2015|Noisy Channel     |ICLR ||[Training Convolutional Networks with Noisy Labels]()
|2015|Noisy Channel     |CVPR |[Caffe](https://github.com/Cysu/noisy_label) | [Learning From Massive Noisy Labeled Data for Image Classification]()
|2015|Sample Choosing   |ICCV |[Website](http://xinleic.xyz/web.html)|[Webly supervised learning of convolutional networks]()
|2015|Sample Weighting  |TPAMI|[Pt](https://github.com/xiaoboxia/Classification-with-noisy-labels-by-importance-reweighting)|[Classification with noisy labels by importance reweighting]()
|2015|Labeler Quality   |     ||[Deep classifiers from image tags in the wild]()
|2015|Robust Losses     |NC   ||[Making risk minimization tolerant to label noise]()
|2015|Robust Losses     |NIPS ||[Learning with symmetric label noise: The importance of being unhinge]()
|2015|MIL               |CVPR ||[Visual recognition by learning from web data: A weakly supervised domain generalization approach]()
|2015|Ensemble Learning |     ||[Rboost: label noise-robust boosting algorithm based on a nonconvex loss function and the numerically stable base learners]()
|2015|Other             |     ||[Learning discriminative reconstructions for unsupervised outlier removal]()
|2016|Noisy Channel     |CASSP|[Keras](https://github.com/udibr/noisy_labels) |[Training deep neural-networks based on unreliable labels]()
|2016|Noisy Channel     |ICDM |[Matlab](https://github.com/ijindal/Noisy_Dropout_regularization)|[Learning deep networks from noisy labels with dropout regularization]()
|2016|Sample Choosing   |ECCV |[Repo](https://github.com/google/goldfinch)|[The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition]()
|2016|Sample Weighting  |CVPR |[Caffe](https://github.com/imisra/latent-noise-icnm)|[Seeing Through the Human Reporting Bias: Visual Classifiers From Noisy Human-Centric Labels]()
|2016|Sample Weighting  |ICLR |[Matlab](https://github.com/azadis/AIR)|[Auxiliary image regularization for deep cnns with noisy labels]()
|2016|Robust Losses     |ICML ||[Loss factorization, weakly supervised learning and label noise robustness]()
|2016|Robust Losses     |     ||[On the convergence of a family of robust losses for stochastic gradient descent]()
|2016|Meta Learning     |NC   ||[Noise detection in the Meta-Learning Level]()
|2016|SSL               |AAAI ||[Robust semi-supervised learning through label aggregation]()
|2016|Regularizer       |CVPR ||[Rethinking the inception architecture for computer vision]()
|2016|Ensemble Learning |KBS  ||[A robust multi-class AdaBoost algorithm for mislabeled noisy data]()
|2017|Noisy Channel     |ICLR |[Keras](https://github.com/udibr/noisy_labels)|[Training Deep Neural-Networks Using a Noise Adaptation Layer]()
|2017|Noisy Channel     |CVPR |[Keras](https://github.com/giorgiop/loss-correction)|[Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach]()
|2017|Noise Cleansing   |CVPR ||[Learning From Noisy Large-Scale Datasets With Minimal Supervision]()
|2017|Noise Cleansing   |     ||[Fidelity-weighted learning]()
|2017|Noise Cleansing   |IEEE ||[Improving crowdsourced label quality using noise correction]()
|2017|Noise Cleansing   |IEEE ||[Self-Error-Correcting Convolutional Neural Network for Learning with Noisy Labels]()
|2017|Sample Choosing   |NIPS ||[Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks]()
|2017|Sample Choosing   |     |[Sklearn](https://github.com/cgnorthcutt/rankpruning)|[Learning with confident examples: Rank pruning for robust classification with noisy labels]()
|2017|Sample Choosing   |     |[Tf](https://github.com/google/mentornet)|[MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels]()
|2017|Sample Choosing   |NIPS |[Tf*](https://github.com/songhwanjun/ActiveBias)|[Active bias: Training more accurate neural networks by emphasizing high variance samples]()
|2017|Sample Choosing   |NIPS |[Tf](https://github.com/emalach/UpdateByDisagreement)|[Decoupling" when to update" from" how to update"]()
|2017|Labeler Quality   |CVPR ||[Lean crowdsourcing: Combining humans and machines in an online system]()
|2017|Labeler Quality   |ICLR ||[Who Said What: Modeling Individual Labelers Improves Classification]()
|2017|Robust Losses     |AAAI ||[Robust Loss Functions under Label Noise for Deep Neural]()
|2017|Meta Learning     |     ||[Learning to Learn from Weak Supervision by Full Supervision]()
|2017|Meta Learning     |     ||[Avoiding your teacher's mistakes: Training neural networks with controlled weak supervision]()
|2017|Meta Learning     |ICCV ||[Learning from Noisy Labels with Distillation]()
|2017|MIL               |CVPR ||[Attend in groups: a weakly-supervised deep learning framework for learning from web data]()
|2017|Regularizer       |     |[Pt](https://github.com/facebookresearch/mixup-cifar10)|[mixup: Beyond Empirical Risk Minimization]()
|2017|Regularizer       |     ||[Regularizing neural networks by penalizing confident output distributions]()
|2017|Other             |     ||[Learning with Auxiliary Less-Noisy Labels]()
|2018|Noisy Channel     |NIPS |[Tf](https://github.com/bhanML/Masking) | [Masking: A new perspective of noisy supervision]()
|2018|Noisy Channel     |IEEE ||[Deep learning from noisy image labels with quality embedding]()
|2018|Noisy Channel     |ISBI ||[Training a neural network based on unreliable human annotation of medical images]()
|2018|Noisy Channel     |NIPS |[Pt](https://github.com/mmazeika/glc)|[Using trusted data to train deep networks on labels corrupted by severe noise]()
|2018|Noise Cleansing   |WACV ||[Iterative cross learning on noisy labels]()
|2018|Noise Cleansing   |TIFS | [Pt](https://github.com/AlfredXiangWu/LightCNN), [Caffe](https://github.com/AlfredXiangWu/face_verification_experiment), [Tf](https://github.com/yxu0611/Tensorflow-implementation-of-LCNN)|[A light CNN for deep face representation with noisy labels]()
|2018|Noise Cleansing   |CVPR | [Chainer](https://github.com/DaikiTanaka-UT/JointOptimization)|[Joint Optimization Framework for Learning with Noisy Labels]()
|2018|Sample Choosing   |ECCV |[Sklearn](https://github.com/MalongTech/research-curriculumnet)|[Curriculumnet: Weakly supervised learning from large-scale web images]()
|2018|Sample Choosing   |IEEE ||[Progressive Stochastic Learning for Noisy Labels]()
|2018|Sample Choosing   |NIPS |[Pt](https://github.com/bhanML/Co-teaching)|[Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels]()
|2018|Sample Weighting  |IEEE ||[Multiclass Learning with Partially Corrupted Labels]()
|2018|Sample Weighting  |     ||[ChoiceNet: Robust Learning by Revealing Output Correlations]()
|2018|Sample Weighting  |CVPR |[Tf](https://github.com/kuanghuei/clean-net)|[Cleannet: Transfer Learning for Scalable Image Classifier Training with Label Noise]()
|2018|Sample Weighting  |     |[Tf](https://github.com/uber-research/learning-to-reweight-examples)|[Learning to Reweight Examples for Robust Deep Learning]()
|2018|Sample Weighting  |CVPR |[Repo](https://github.com/YisenWang/Iterative_learning)|[Iterative Learning With Open-Set Noisy Labels]()                       
|2018|Labeler Quality   |AAAI ||[Deep learning from crowds]()
|2018|Labeler Quality   |ICLR |[Repo](https://github.com/khetan2/MBEM)|[Learning From Noisy Singly-Labeled Data]()
|2018|Robust Losses     |NIPS ||[Generalized cross entropy loss for training deep neural networks with noisy labels]()
|2018|Meta Learning     |     ||[Improving Multi-Person Pose Estimation using Label Correction]()
|2018|SSL               |WACV ||[A semi-supervised two-stage approach to learning from noisy labels]()
|2018|Regularizer       |ECCV ||[Deep bilevel learning]()
|2018|Regularizer       |ICLR |[Keras](https://github.com/xingjunm/dimensionality-driven-learning)|[Dimensionality Driven Learning for Noisy Labels]()
|2018|Other             |     ||[Robust Determinantal Generative Classifier for Noisy Labels and Adversarial Attacks]()
|2018|Other             |ECCV ||[Learning with Biased Complementary Labels]()
|2019|Noisy Channel     |ICML |[Pt](https://github.com/PaulAlbert31/LabelNoiseCorrection)|[Unsupervised Label Noise Modeling and Loss Correction]()
|2019|Noisy Channel     |AAAI |[Tf](https://github.com/Sunarker/Safeguarded-Dynamic-Label-Regression-for-Noisy-Supervision)|[Safeguarded Dynamic Label Regression for Noisy Supervision]()
|2019|Noise Cleansing   |ICCV ||[Deep Self-Learning From Noisy Labels]()
|2019|Noise Cleansing   |TGRS |[Matlab](https://github.com/junjun-jiang/RLPA)|[Hyperspectral image classification in the presence of noisy labels]()
|2019|Noise Cleansing   |CVPR |[Pt](https://github.com/yikun2019/PENCIL)|[Probabilistic End-to-end Noise Correction for Learning with Noisy Labels]()
|2019|Noise Cleansing   |ICCV ||[Photometric Transformer Networks and Label Adjustment for Breast Density Prediction]()
|2019|Noise Cleansing   |CVPR |[Pt](https://github.com/jx-zhong-for-academic-purpose/GCN-Anomaly-Detection)|[Graph Convolutional Label Noise Cleaner: Train a Plug-and-play Action Classifier for Anomaly Detection]()
|2019|Sample Choosing   |     ||[SELF: Learning to Filter Noisy Labels with Self-Ensembling]()
|2019|Sample Choosing   |     ||[Curriculum Loss: Robust Learning and Generalization against Label Corruption]()
|2019|Sample Choosing   |CVPR ||[Learning a Deep ConvNet for Multi-label Classification with Partial Labels]()
|2019|Sample Choosing   |ICML |[Pt](https://github.com/bhanML/coteaching_plus)|[How does Disagreement Help Generalization against Label Corruption?]()
|2019|Sample Choosing   |ICML |[Keras](https://github.com/chenpf1025/noisy_label_understanding_utilizing)|[Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels]()
|2019|Sample Weighting  |     ||[Robust Learning at Noisy Labeled Medical Images: Applied to Skin Lesion Classification]()
|2019|Sample Weighting  |ICML |[Pt](https://github.com/thulas/dac-label-noise)|[Combating Label Noise in Deep Learning Using Abstention]()
|2019|Sample Weighting  |CVPR |[Caffe](https://github.com/huangyangyu/NoiseFace)|[Noise-Tolerant Paradigm for Training Face Recognition CNNs]()
|2019|Labeler Quality   |CVPR ||[Learning From Noisy Labels By Regularized Estimation Of Annotator Confusion]()
|2019|Robust Losses     |     ||[Improved Mean Absolute Error for Learning Meaningful Patterns from Abnormal Training Data]()
|2019|Robust Losses     |     ||[Symmetric Cross Entropy for Robust Learning with Noisy Labels]()
|2019|Meta Learning     |     ||[Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels]()
|2019|Meta Learning     |CVPR |[Pt](https://github.com/LiJunnan1992/MLNT)|[Learning to Learn from Noisy Labeled Data]()
|2019|SSL               |     ||[Robust Learning Under Label Noise With Iterative Noise-Filtering]()
|2019|Regularizer       |     |[Pt](https://github.com/hendrycks/pre-training)|[Using Pre-Training Can Improve Model Robustness and Uncertainty]()
|2019|Other             |     ||[NLNL: Negative Learning for Noisy Labels]()

List of papers that shed light to label noise phenomenon for deep learning:
|Title                                                                                                      | Year |
|-----                                                                                                      | ---- |
|[Class noise vs. attribute noise: A quantitative study]()                                                  | 2004 |
|[Class noise and supervised learning in medical domains: The effect of feature extraction]()               | 2006 |
|[Classification in the Presence of Label Noise: a Survey]()                                                | 2014 |
|[A comprehensive introduction to label noise]()                                                            | 2014 |
|[Understanding deep learning requires rethinking generalization]()                                         | 2016 |
|[A study of the effect of different types of noise on the precision of supervised learning techniques]()   | 2017 |
|[On the robustness of convnets to training on noisy labels]()                                              | 2017 |
|[Deep Nets Don't Learn via Memorization]()                                                                 | 2017 |
|[A closer look at memorization in deep networks]()                                                         | 2017 |
|[Deep Learning is Robust to Massive Label Noise]()                                                         | 2018 |
|[How Do Neural Networks Overcome Label Noise?]()                                                           | 2018 |

List of works under label noise beside classification
|Title                                                                          |Year|
|-----                                                                          |----|
|[Label-Noise Robust Generative Adversarial Networks]()                         |2018|
|[Robustness of conditional GANs to noisy labels]()                             |2018|
|[Learning from weak and noisy labels for semantic segmentation]()              |2016|
|[Improving Semantic Segmentation via Video Propagation and Label Relaxation]() |2018|
|[Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations]() |2019|

Sources on web
* [Noisy-Labels-Problem-Collection](https://github.com/GuokaiLiu/Noisy-Labels-Problem-Collection)
* [Learning-with-Label-Noise](https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise)

Abbreviations
* SSL -> Semi-Supervised Learning
* MIL -> Multiple Instance Learning
* NC -> Neurocomputing
* Tf -> Tensorflow
* Pt -> PyTorch

Starred (*) repos means code is unoffical!